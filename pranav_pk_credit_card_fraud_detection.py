# -*- coding: utf-8 -*-
"""PRANAV PK Credit Card Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16CxByygMfywcTJlugE9Kwe0ZqAdkIZwo

In this notebook, I will predict fraud transactions from a given dataset. Since the data is imbalanced, using standard metrics like accuracy to evaluate the classification algorithm would be misleading. Instead, I'll focus on metrics like Sensitivity (true positive rate) and Specificity (true negative rate). These two metrics are interdependent, so the goal is to find an optimal trade-off between them. The ideal balance depends on the specific application, and in the case of fraud detection, I’ll prioritize high Sensitivity—I want to detect fraudulent transactions with high probability

**IMPORTING LIBRARIES:**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pylab import rcParams
import warnings
warnings.filterwarnings('ignore')

"""**READING DATASET :**"""

data=pd.read_csv('creditcard.csv')

data.head()

"""**NULL VALUES:**"""

data.isnull().sum()

"""**Thus there are no null values in the dataset.**

**INFORMATION**
"""

data.info()

"""**DESCRIPTIVE STATISTICS**"""

data.describe().T.head()

data.shape

"""**Thus there are 284807 rows and 31 columns.**"""

data.columns

"""**FRAUD CASES AND GENUINE CASES**"""

fraud_cases=len(data[data['Class']==1])

print(' Number of Fraud Cases:',fraud_cases)

non_fraud_cases=len(data[data['Class']==0])

print('Number of Non Fraud Cases:',non_fraud_cases)

fraud=data[data['Class']==1]

genuine=data[data['Class']==0]

fraud.Amount.describe()

genuine.Amount.describe()

"""**EDA**"""

data.hist(figsize=(20,20),color='lime')
plt.show()

rcParams['figure.figsize'] = 16, 8
f,(ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Time of transaction vs Amount by class')
ax1.scatter(fraud.Time, fraud.Amount)
ax1.set_title('Fraud')
ax2.scatter(genuine.Time, genuine.Amount)
ax2.set_title('Genuine')
plt.xlabel('Time (in Seconds)')
plt.ylabel('Amount')
plt.show()

"""**CORRELATION**"""

plt.figure(figsize=(10,8))
corr=data.corr()
sns.heatmap(corr,cmap='BuPu')

"""**Let us build our models:**"""

from sklearn.model_selection import train_test_split

"""**Model 1:**"""

X=data.drop(['Class'],axis=1)

y=data['Class']

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=123)

from sklearn.ensemble import RandomForestClassifier

rfc=RandomForestClassifier()

model=rfc.fit(X_train,y_train)

X_train.dropna(inplace=True)
# Ensure y_train is also updated to match the dropped rows
y_train = y_train[X_train.index]
model=rfc.fit(X_train,y_train)

prediction=model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test,prediction)

"""**Model 2:**"""

from sklearn.linear_model import LogisticRegression

X1=data.drop(['Class'],axis=1)

y1=data['Class']

X1_train,X1_test,y1_train,y1_test=train_test_split(X1,y1,test_size=0.3,random_state=123)

lr=LogisticRegression()

model2=lr.fit(X1_train,y1_train)

X1_train.dropna(inplace=True)
y1_train = y1_train[X1_train.index] # Make sure y1_train matches X1_train after dropping rows
model2 = lr.fit(X1_train, y1_train)

prediction2=model2.predict(X1_test)

accuracy_score(y1_test,prediction2)

"""**Model 3:**"""

from sklearn.tree import DecisionTreeRegressor

X2=data.drop(['Class'],axis=1)

y2=data['Class']

dt=DecisionTreeRegressor()

X2_train,X2_test,y2_train,y2_test=train_test_split(X2,y2,test_size=0.3,random_state=123)

model3=dt.fit(X2_train,y2_train)

# ipython-input-49-9e40dd3bc30f
from sklearn.tree import DecisionTreeRegressor # Make sure to import the necessary module
dt=DecisionTreeRegressor()

# ipython-input-50-9e40dd3bc30f
# Assuming X2 and y2 are defined
X2_train,X2_test,y2_train,y2_test=train_test_split(X2,y2,test_size=0.3,random_state=123)

# ipython-input-55-9e40dd3bc30f
model3=dt.fit(X2_train,y2_train)

# ipython-input-56-9e40dd3bc30f
prediction3=model3.predict(X2_test)

# Check the shapes of X2 and y2
print(X2.shape)
print(y2.shape)

# If there's a mismatch, investigate why and fix the data
# For example, if y2 has an extra row, you might remove it:
y2 = y2[:-1]

# Or if X2 is missing a row, you might add it back

# Once the shapes match, proceed with train_test_split
X2_train,X2_test,y2_train,y2_test=train_test_split(X2,y2,test_size=0.3,random_state=123)

# Drop rows with NaN in 'Class' column from the original DataFrame
data = data.dropna(subset=['Class'])

# Redo the steps to create X2, y2, and the train-test split
X2 = data.drop(['Class'], axis=1)

prediction3=model3.predict(X2_test)

accuracy_score(y2_test,prediction3)

"""**All of our models performed with a very high accuracy.**"""

